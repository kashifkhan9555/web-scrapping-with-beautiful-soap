{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPkinhjPlmAaBEXxTudkNd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashifkhan9555/web-scrapping-with-beautiful-soap/blob/main/web_scrapping_with_beautiful_soap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42tsJ-hpzNRN",
        "outputId": "7a60efcc-3cec-467b-eda8-a833fd6544ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "\n",
        "def scrape_webpage(url, headers):\n",
        "    retries = 3\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            # Fetch HTML content from the URL\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()  # Raise an exception for 4xx and 5xx status codes\n",
        "            html_content = response.text\n",
        "\n",
        "            # Parse HTML using BeautifulSoup\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "            # Extract relevant information\n",
        "            text_content = soup.get_text().replace(\"\\n\", \" \").replace(\",\", \" \")\n",
        "            image_urls = [img[\"src\"] for img in soup.find_all(\"img\", src=True)]\n",
        "            links = [link[\"href\"] for link in soup.find_all(\"a\", href=True)]\n",
        "\n",
        "            # Store scraped data in a structured format (dictionary)\n",
        "            data = {\n",
        "                \"URL\": url,\n",
        "                \"Text Content\": text_content,\n",
        "                \"Image URLs\": \", \".join(image_urls),\n",
        "                \"Links\": \", \".join(links)\n",
        "            }\n",
        "            return data\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            print(f\"Failed to fetch HTML content from {url}. Status code: {response.status_code}\")\n",
        "            if response.status_code in [403, 429, 520]:\n",
        "                # Implement a backoff strategy for 403 Forbidden, 429 Too Many Requests, and 520 Origin Error\n",
        "                if _ == retries - 1:\n",
        "                    print(f\"Skipping {url} after maximum retries.\")\n",
        "                    return None\n",
        "                print(f\"Retrying {url} in 60 seconds...\")\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                break\n",
        "        except (requests.exceptions.RequestException, Exception) as e:\n",
        "            print(f\"An error occurred while scraping {url}: {e}\")\n",
        "            if _ == retries - 1:\n",
        "                print(f\"Skipping {url} after maximum retries.\")\n",
        "                return None\n",
        "            # Implement a backoff strategy for other exceptions\n",
        "            print(f\"Retrying {url} in {2 ** _ + random.uniform(0, 1)} seconds...\")\n",
        "            time.sleep(2 ** _ + random.uniform(0, 1))\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    # List of URLs to scrape\n",
        "    urls = [\n",
        "        \"https://products.basf.com/global/en/ci/n-vinyl-2-pyrrolidone.html\",\n",
        "        \"https://pubchem.ncbi.nlm.nih.gov/compound/N-Vinyl-2-pyrrolidone\",\n",
        "        \"https://www.shokubai.co.jp/en/products/detail/nvp/\",\n",
        "        \"https://pubchem.ncbi.nlm.nih.gov/compound/N-Vinyl-2-pyrrolidone\",\n",
        "        \"https://www.sciencedirect.com/topics/pharmacology-toxicology-and-pharmaceutical-science/1-vinyl-2-pyrrolidinone\",\n",
        "        \"https://www.ncbi.nlm.nih.gov/books/NBK498761/#:~:text=It%20is%20used%20in%20the,the%20synthesis%20of%20phenolic%20resins\",\n",
        "        \"https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/polyvinylpyrrolidone#:~:text=PVP%20added%20to%20iodine%20forms,trade%20name%20Betadine%20and%20Pyodine\",\n",
        "        \"https://www.shokubai.co.jp/en/products/detail/nvp/#:~:text=N%2Dvinylpyrrolidone%20is%20a%20nonionic,monomer%20with%20the%20following%20features.&text=N%2Dvinylpyrrolidone%20is%20used%20as,of%20reactivity%20with%20UV%20irradiation\",\n",
        "        \"https://adhesives.specialchem.com/product/m-basf-n-vinyl-pyrrolidone-nvp\",\n",
        "        \"https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/\",\n",
        "        \"https://pubs.rsc.org/en/content/articlelanding/2019/py/c8py01459k\",\n",
        "        \"https://www.science.gov/topicpages/n/n-vinyl+pyrrolidone+nvp\",\n",
        "        \"https://shdexiang.en.made-in-china.com/product/tXfQDioPsKVn/China-N-Vinylpyrrolidone-CAS-No-88-12-0-C6h9no.html\",\n",
        "        \"https://www.cphi-online.com/nvp-n-vinylpyrrolidone-prod1288298.html\",\n",
        "        \"https://www.mdpi.com/2073-4360/11/6/1079\"\n",
        "    ]\n",
        "\n",
        "    # Rotate user-agent headers\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:53.0) Gecko/20100101 Firefox/53.0\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4\"\n",
        "    ]\n",
        "\n",
        "    # Scrape data from each URL\n",
        "    scraped_data = []\n",
        "    for url in urls:\n",
        "        headers = {\"User-Agent\": random.choice(user_agents)}\n",
        "        data = scrape_webpage(url, headers)\n",
        "        if data:\n",
        "            scraped_data.append(data)\n",
        "\n",
        "    # Save scraped data to a CSV file\n",
        "    output_file_path = \"scraped_data.csv\"\n",
        "    with open(output_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "        fieldnames = [\"URL\", \"Text Content\", \"Image URLs\", \"Links\"]\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(scraped_data)\n",
        "\n",
        "    print(f\"Scraped data saved to '{output_file_path}' in CSV format.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxRTHadKzVjx",
        "outputId": "6386fa10-e6b2-42bd-fac7-3ddb56ae3479"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch HTML content from https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/. Status code: 520\n",
            "Retrying https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/ in 60 seconds...\n",
            "Failed to fetch HTML content from https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/. Status code: 520\n",
            "Retrying https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/ in 60 seconds...\n",
            "Failed to fetch HTML content from https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/. Status code: 520\n",
            "Skipping https://www.welinkschem.com/nvp-n-vinyl-pyrrolidone/ after maximum retries.\n",
            "Scraped data saved to 'scraped_data.csv' in CSV format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1ghaJymzVhA",
        "outputId": "818a1110-bddb-4bb8-fabe-0bd4631c2f90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'scraped_data.csv', 'scraped_data.json', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('scraped_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sLBg77XMzVeo",
        "outputId": "615b4f3a-9db6-4dd4-c9fb-3bbc156c5952"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be0baee3-e10e-4554-9d17-3ccdd8eb8ca4\", \"scraped_data.csv\", 932004)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}